{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276c0073",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "__MATH 3480__ - Dr. Michael Olson\n",
    "\n",
    "Reading:\n",
    "* Geron, Chapter 5\n",
    "* Geron, Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b40333-904b-4c99-b9e9-fe8e2d2d994f",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89807d-61f4-49c1-b2b9-3f51df3d6434",
   "metadata": {},
   "source": [
    "* The concept of a neural network was first theorized in 1943\n",
    "* Technique of the Perceptron was introduced in 1958 by Frank Rosenblatt\n",
    "  * Belief that ANNs (Artificial Neural Networks) would soon be able to translate languages on the fly\n",
    "* Due to lack in technology and in data, funding failed and focuses shifted to other methods\n",
    "  * \"AI Winter\"\n",
    "* ImageNet 2012 - Large Scale Visual Recognition Challenge\n",
    "  * Reintroduced Neural Networks\n",
    " \n",
    "Today, systems are using ANNs everywhere (e.g. Apple and Google for speech and image recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374c9f7-c000-4a4d-8bbb-fb0a21ce2560",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15175541-2b9e-4d4a-9cdd-8aff73ccb6b7",
   "metadata": {},
   "source": [
    "In a one-layer neural network, also known as a __perceptron__, only does one calculation. There are two parts:\n",
    "* An input layer\n",
    "* An output layer (where the calculation happens)\n",
    "\n",
    "The value of the perceptron is some function of the input layer. For example,\n",
    "$$y=x_1 + x_2 + \\dots + x_n$$\n",
    "\n",
    "<img src=\"https://github.com/drolsonmi/math3480/blob/main/Notes/Images/NN_01Perceptron.png?raw=true\" width=350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a859fcb8-6c39-41c6-aac5-e53713900f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5, 13]), array([4, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([5,13])\n",
    "x2 = np.array([4,1])\n",
    "x1,x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07927f47-8c7a-4149-bad2-5ef3eda23e93",
   "metadata": {},
   "source": [
    "Sometimes, the value is not a direct sum. It is often a weighted sum, and the goal of the perceptron is to find the right weights.\n",
    "$$y = w_1x_1 + w_2x_2 + \\dots + w_nx_n$$\n",
    "\n",
    "<img src=\"https://github.com/drolsonmi/math3480/blob/main/Notes/Images/NN_02PerceptronWeighted.png?raw=true\" width=350>\n",
    "\n",
    "But what if all $x_i=0$? Then the weights won't do anything and the perceptron becomes dead or useless. So, to prevent a zero value, we have to add in a bias term. This bias term can be positive or negative, and it can be large or small.\n",
    "$$y = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b$$\n",
    "$$y = \\sum_i w_i x_i + b$$\n",
    "\n",
    "<img src=\"https://github.com/drolsonmi/math3480/blob/main/Notes/Images/NN_03LinReg.png?raw=true\" width=350>\n",
    "\n",
    "<font color=red>__INSERT SECTION ON MLPs (MULTILAYER PERCEPTRONS)__</font>\n",
    "\n",
    "Notice how this is a lot like a linear regression. It is just a version of,\n",
    "$$AX=Y \\qquad \\left[a_1,a_2,\\dots,a_n\\right]\\begin{bmatrix}\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "x_1 & x_2 & \\dots & x_n \\\\\n",
    "\\vdots & \\vdots & & \\vdots\n",
    "\\end{bmatrix}=\\left[y_1,y_2,\\dots,y_n\\right]$$\n",
    "\n",
    "<img src=\"https://github.com/drolsonmi/math3480/blob/main/Notes/Images/NN_04MLP.png?raw=true\" width=350>\n",
    "\n",
    "<img src=\"https://github.com/drolsonmi/math3480/blob/main/Notes/Images/NN_05MLP2.png?raw=true\" width=350>\n",
    "\n",
    "So, at this point, this really is just a simple linear algebra problem, which we could solve using the Pseudoinverse:\n",
    "$$A=YX^\\dagger$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0a0c5-9531-49b1-9c01-d3ec85a35379",
   "metadata": {},
   "source": [
    "This is the basic idea of the function of a perceptron. The result would be the value, or __activation__, of the output neuron(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c74678-9854-4f14-85dc-ba4778429bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####   LOAD THE DATA   #####\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data#[:, (2,3)] # petal length, petal width\n",
    "#y = (iris.target == 0).astype(np.int) # Iris setosa\n",
    "y = iris.target\n",
    "\n",
    "#####   CROSS VALIDATION   #####\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=20)\n",
    "\n",
    "#####   TRAIN THE PERCEPTRON MODEL   #####\n",
    "from sklearn.linear_model import Perceptron\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X_train,y_train)\n",
    "\n",
    "#####   PREDICT FOR THE TEST DATA   #####\n",
    "y_pred = per_clf.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "#####   PLOT   #####\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "X_df = pd.DataFrame(X, columns=['Sepal Length','Sepal Width','Petal Length','Petal Width'])\n",
    "X_df['Species'] = pd.DataFrame(y)\n",
    "sns.scatterplot(data=X_df, x='Petal Length', y='Petal Width', hue='Species', ax=ax1)\n",
    "ax1.set_title('Original Data')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "Xtest_df = pd.DataFrame(X_test, columns=['Sepal Length','Sepal Width','Petal Length','Petal Width'])\n",
    "Xtest_df['Species'] = pd.DataFrame(y_pred)\n",
    "sns.scatterplot(data=Xtest_df, x='Petal Length', y='Petal Width', hue='Species', ax=ax2)\n",
    "ax2.set_title('Testing Data')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#####   EVALUATION   #####\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6207ac1-a394-430d-81e2-c292fad30bd3",
   "metadata": {},
   "source": [
    "Two issues:\n",
    "1. Need more data to train the model (80% * 150 = 120)\n",
    "2. This is basically a multi-linear regression - we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c75f47-d857-4b49-9ff9-c6ed9bcbfc65",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7c45-ac6c-4def-adf2-d5f1611fa6d7",
   "metadata": {},
   "source": [
    "Now, what do we want the output of our activated neuron to look like?\n",
    "* A value (regression)\n",
    "* A 1 or a 0 (hard classification)\n",
    "* A probability (soft classification)\n",
    "\n",
    "To get the activation node to the activated value we want, we are going to take our weighted calculations and send them through a specific kind of function which we call an __activation function__.\n",
    "\n",
    "$$\\hat{y} = \\sigma(z) \\qquad z = \\sum_i w_ix_i + b$$\n",
    "\n",
    "The type of activation function we use depends on the need. Up until now, we have only dealt with a linear function.\n",
    "\n",
    "#### Linear function\n",
    "$$\\hat{y} = \\sigma(z) = z = \\sum_i w_ix_i + b$$\n",
    "\n",
    "However, activation functions are not always linear. Often, we want it to be a number between 0 and 1. Here are a few nonlinear activation functions:\n",
    "\n",
    "#### Step function\n",
    "Also known as the *Threshold Logic Unit (TLU)* or *Linear Threshold Unity (LTU)*\n",
    "\n",
    "Utilizes $H(z)$, the Heaviside Step Function\n",
    "\n",
    "$$\\hat{y} = \\sigma(z) = H(z) = \\left\\{\\begin{align*}1 & \\text{ if }z \\ge 0 \\\\ 0 & \\text{ if }z < 0\\end{align*}\\right. \\qquad z = \\sum_i w_ix_i + b$$\n",
    "\n",
    "![Step Curve](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Activation_binary_step.svg/120px-Activation_binary_step.svg.png)\n",
    "\n",
    "#### Sigmoid function (or Logical function)\n",
    "  * More sensitive to small changes\n",
    "$$\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-z}} \\qquad z = \\sum_i w_ix_i + b$$\n",
    "![Sigmoid Curve](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/120px-Activation_logistic.svg.png)\n",
    "\n",
    "#### Hyperbolic Tangent\n",
    "  * This one acts like the Sigmoid function, but goes from -1 to 1\n",
    "$$\\tanh(z)=\\frac{e^z - e^{-z}}{e^z + e^{-z}} \\qquad z = \\sum_i w_ix_i + b$$\n",
    "![Hyperbolic Tangent Curve](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/120px-Activation_tanh.svg.png)\n",
    "\n",
    "#### Rectified Linear Unit (ReLU)\n",
    "  $$y=\\max\\{0,z\\} \\qquad z = \\sum_i w_ix_i + b$$\n",
    "![ReLU Curve](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/120px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "Commonly used in hidden layers to solve the *vanishing gradient* problem.\n",
    "* Discuss more later when we talk about traning neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e532c00-afb1-4c8e-a78e-4db2a4a151df",
   "metadata": {},
   "source": [
    "There are many other activation functions. However, in order to train our network, our activation function needs to be differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc893b05-670a-482f-a7c4-a86bfca05331",
   "metadata": {},
   "source": [
    "## Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ade1c-9785-40ba-9e87-7d2343ba7b1d",
   "metadata": {},
   "source": [
    "Knowing the basic perceptron, we can solve basic problems. But we can do little better than an ordinary linear regression. We can improve performance by adding a second or third step before getting our prediction $\\hat{y}$. These extra layers between the input layer and output layer are called __hidden layers__, and a basic neural network with 1 or more hidden layers is a __multi-layer perceptron__, a basic __artificial neural network__.\n",
    "\n",
    "### Some basic terminology\n",
    "* The __width of a layer__ is the number of neurons within that layer\n",
    "* The __depth of a network__ is the number of (*Hidden?*) layers in a network\n",
    "* An ANN becomes a __Deep Neural Network__ (DNN) when there are 2 or more hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1409d98-e09b-4671-a931-04f0e8b20f25",
   "metadata": {},
   "source": [
    "## Training an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161499-a9e0-4bec-ab10-669c4bb130df",
   "metadata": {},
   "source": [
    "An ANN most often doesn't get it right the first time. The program has to iterate through the network multiple times to get it right. Each iteration updates the activators (neurons) to improve the result. The activators are updated with new weights in a variety of ways.\n",
    "\n",
    "The most common way to update weights in an activator is via gradient descent:\n",
    "$$w_{ij}'=w_{ij} + \\eta\\left(y_i - \\hat{y}_i\\right)x_i$$\n",
    "where\n",
    "* $w_{ij}$ is the current weight, and $w_{ij}'$ is the updated weight\n",
    "* $\\eta$ is the learning rate\n",
    "* $x_i$ is the input\n",
    "* $y_i$ is the target output\n",
    "* $\\hat{y}_i$ is the actual output using the weight of $w_{ij}$\n",
    "\n",
    "This is the algorithm developed by Frank Rosenblatt in 1958, inspired by *Hebb's Rule*, which was published in 1949. \n",
    "\n",
    "#### More on ReLU Activation Function\n",
    "When we have a DNN, the gradient descent algorithm has to be performed at each stage. However, this poses a problem. The *backpropagation* method of training a DNN (think very intense process of doing gradient descent) uses the chain rule to capture errors between steps. The chain rule requires us to multiply our derivatives together, which is where the problem arises.\n",
    "\n",
    "Imagine you have a derivative of 0.6 at each of 7 steps. This indicates a large error. But because of the chain rule, we multiply these together and we get $0.6^7 = 0.028$, which is a small error.\n",
    "\n",
    "To prevent this, we limit how low we can go. The ReLU activation function addresses this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3bb65-4d90-4ec9-8fc4-509d3c3b0d78",
   "metadata": {},
   "source": [
    "## ANNs Today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8ae48-8389-445c-9a3d-2a17e2b27e62",
   "metadata": {},
   "source": [
    "The result of these neural networks are quite good.\n",
    "* [Mona Lisa can talk!](https://www.youtube.com/watch?v=P2uZF-5F1wI)\n",
    "* [Mark Rober - Stealing Baseball Signs with a Phone](https://www.youtube.com/watch?v=PmlRbfSavbI)\n",
    "\n",
    "Are Neural Networks perfect yet? No...\n",
    "* [Neurabites.com](https://neurabites.com/muffin-or-chihuahua/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67474af0-7a74-42ec-9718-f98120da43bd",
   "metadata": {},
   "source": [
    "### Cautions with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5bb7c-9007-473a-a824-695a232e77f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "* ANNs are relatively easy to train. However, because it's so easy, many people are just using ANNs without understanding what is happening under the hood.\n",
    "\n",
    "Some of the issues:\n",
    "* Easy to overfit\n",
    "  * There are some cases where we want very specific results, but it's very easy to overfit\n",
    "* Not very good when we want to generalize things\n",
    "  * Not very good with learning physics (model it to predict where a cannonball lands, but it can't use that to launch a rocket)\n",
    "* Interpretable/Explainable\n",
    "  * The number of neurons adds a large number of degrees of freedom which makes it very expressive\n",
    "  * More degrees of freedom makes it more complicated and harder to interpret\n",
    "* Incorporating Physics\n",
    "  * It is a difficult process to incorporate our knowledge of the real world\n",
    "  * True for all ML Algorithms, but especially for ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c86f6",
   "metadata": {},
   "source": [
    "## Building an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9b524",
   "metadata": {},
   "source": [
    "To build an ANN, we first create our input and output layers.\n",
    "\n",
    "### Input\n",
    "How many datapoints are going into the model? \n",
    "\n",
    "In the MNIST dataset, each image had a resolution of 28x28, or a total of 28*28=784 pixels. So, our input layer has a width of 784, one node for each pixel.\n",
    "\n",
    "### Output\n",
    "What is the desired output? How many nodes are needed to represent that output?\n",
    "\n",
    "In the MNIST dataset, the images are of numerical digits. So, the output is going to be any of the 10 digit. If we create 10 nodes (one for each digit), then we can use a Logistic/Softmax activation function to get a probability between 0 and 1. So, our output layer will have a width of 10.\n",
    "\n",
    "### Hidden layers\n",
    "Now, we determine the hidden layers. We have to decide how many hidden layers are needed and the width of each hidden layer.\n",
    "\n",
    "#### Number of hidden layers\n",
    "One hidden layer is generally enough, but deep neural networks (more than one hidden layer) have a higher parameter efficiency. Deep networks can use exponentially fewer neurons than shallow networks. Some rules of thumb:\n",
    "* lower hidden layers (layers near the input) model low-level structures (e.g., line segments of various shapes and orientations)\n",
    "* intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles)\n",
    "* Higher hidden layers (layers near the output) model high-level structures (e.g., faces)\n",
    "\n",
    "*Transfer learning*: using lower layers from one model in another situation. For example, using the lower layers in the above example to recognize line segments, then build a network to identify animals instead of faces.\n",
    "\n",
    "#### Width of hidden layers\n",
    "Early models used a pyramid structure - larger layers leading to gradually smaller layers. Experience has shown this really doesn't perform any better than layers with the same number of neurons. In fact, equally-sized layers tend to perform slightly better than decreasing layers.\n",
    "\n",
    "Generally, we do start with a larger first hidden layer, then shrink to a smaller layer and keep other hidden layers roughly the same size.\n",
    "\n",
    "In the past, programmers would retrain NNs, gradually increasing the number of neurons, until the model starts to overfit. More modern models start with large numbers of neurons and use early-stopping techniques to prevent the models from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfa99",
   "metadata": {},
   "source": [
    "## Example with Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763fb40",
   "metadata": {},
   "source": [
    "* Create a new virtual environment for Tensorflow and Keras\n",
    "* Install Tensorflow and Keras\n",
    "* Check for the version of Tensorflow and Keras that you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaae6e",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35531cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# There are 60,000 images of size 28x28\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60347049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE! Each pixel of the image is represended as a value from 0 to 255.\n",
    "# Two problems\n",
    "   # We want a value from 0 to 1\n",
    "   # It is an integer, not a float value\n",
    "# To fix both, divide by 255.0\n",
    "\n",
    "\n",
    "# We have a test set, but we need a validation set:\n",
    "X_valid, X_train = X[:5000] / 255.0 , X[5000:] / 255.0\n",
    "y_valid, y_train = y[:5000], y[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb72bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "[class_names[y_train[i]] for i in range(20) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b3228",
   "metadata": {},
   "source": [
    "#### Plot the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = ['ax1','ax2','ax3','ax4','ax5','ax6','ax7','ax8','ax9','ax10','ax11','ax12','ax13','ax14','ax15','ax16','ax17','ax18','ax19','ax20']\n",
    "f, ax = plt.subplots(1, 20, sharey=True, figsize=(25,6))\n",
    "for i in range(20):\n",
    "    img = ax[i].imshow(X_train[i])\n",
    "    cmap = plt.cm.get_cmap('gray_r')\n",
    "    img.set_cmap(cmap)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e0a34",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()                        # Input layer - Single stack of layers\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))     # Some Preprocessing - converts each input image into a 1D array\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))    # Hidden layer\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))    # Hidden layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is the same as the previous cell\n",
    "\n",
    "from keras.layers import Dense\n",
    "model = keras.models.Sequential([                        # Input layer - Single stack of layers\n",
    "    keras.layers.Flatten(input_shape=[28,28]),           # Some Preprocessing - converts each input image into a 1D array\n",
    "    Dense(300, activation=\"relu\"),                       # Hidden layer\n",
    "    Dense(100, activation=\"relu\"),                       # Hidden layer\n",
    "    Dense(10, activation=\"softmax\")                      # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# \"Param #\" is the number of weights and biases leading into that layer\n",
    "# The first layer has 784 neurons\n",
    "# That is 784*300 connections, so 784*300 weights and 300 biases\n",
    "# total of 235200 + 300 = 235500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the layers\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values for all weights leading to a layer\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21643c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb69b1c",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5972a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",                           # sgd = Stochastic Gradient Descent\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9337fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a021c81",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ['ax1','ax2','ax3']\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(6,6))\n",
    "for i in range(3):\n",
    "    img = ax[i].imshow(X_test[i])\n",
    "    cmap = plt.cm.get_cmap('gray_r')\n",
    "    img.set_cmap(cmap)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classes:  \n",
    "# [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "y_prob = model.predict(X_test[:3])\n",
    "y_prob.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values for all weights leading to a layer\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f819ad",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a model for future use\n",
    "model.save(\"FashionMNIST.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd56e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a model that was previously saved\n",
    "loaded_model = keras.models.load_model('FashionMNIST.h5')\n",
    "\n",
    "# Then, use as normal\n",
    "starting_image = 17\n",
    "y_prob = loaded_model.predict(X_test[starting_image:starting_image+3])\n",
    "print(y_prob.round(2))\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(6,6))\n",
    "for i in range(3):\n",
    "    img = ax[i].imshow(X_test[starting_image+i])\n",
    "    cmap = plt.cm.get_cmap('gray_r')\n",
    "    img.set_cmap(cmap)\n",
    "    ax[i].axis('off')\n",
    "\n",
    "### Classes:  \n",
    "# [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
